\documentclass[12pt,english]{article}
\usepackage{geometry}
\geometry{a4paper,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
\usepackage{babel}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{amsmath}

\begin{document}

%%%%%%%%%%%%%
% TITLE/TOC %
%%%%%%%%%%%%%

\title{Advanced Operating Systems - System Documentation}
\author{\textbf{Group 3} \\ David Terei, Benjamin Kalman}
\maketitle

\tableofcontents{}

%%%%%%%%
% TODO %
%%%%%%%%

\newpage{}
\section{TODO}

\begin{itemize}
\item 3: system call interface
\item 4: NFS
\item 5: demand paging
\item 6: timer driver?
\item 7: process management
\item 8: ELF loading
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%
% MEMORY MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%

\section{Memory management}

\subsection{Page table structure}

The page table is a lazy, two level page table.  

\begin{verbatim}
#define PAGEWORDS (PAGESIZE / sizeof(L4_Word_t))

typedef struct  {
    L4_Word_t pages[PAGEWORDS];
} Pagetable2;

typedef struct {
    Pagetable2 *pages2[PAGEWORDS];
} Pagetable1;
\end{verbatim}

\subsection{Address location}

To locate the 32 bit virtual address \texttt{v}, the highest 12 bits are used to index in to the first level of the table (\texttt{Pagetable1}) in order to find the corresponding second level struct.  If this is \texttt{NULL}, this second level is dynamically created.  The middle 12 bits of \texttt{v} are then used to index in to this second level, locating the relevant entry \texttt{e}.  Note that the lowest 12 bits of \texttt{v} are not needed for page table addressing.

The high 24 bits of \texttt{e} give the \emph{page aligned} physical address\footnote{In fact, ppage} of what \texttt{v} is mapped to.  The lower 12 bits are then used to maintain status bits, for example the reference bit and swapped status.

\subsection{Region management}

Regions are managed with a linked list of \texttt{Region} structs.

\begin{verbatim}
struct Region_t {
    region_type type;
    uintptr_t base;
    unsigned int size;
    unsigned int filesize;
    int rights;
    int mapDirectly;
    Swapfile *elffile;
};
\end{verbatim}

\begin{itemize}
\item \texttt{type} is used to differentiate between the stack, heap, and other regions.  It is important to know which region is the heap in order for the \texttt{moremem} system call to function, see Section \ref{sub:moremem}.
\item \texttt{base}, \texttt{size}, and \texttt{rights} are the self explanitory.
\item \texttt{filesize} is the size of the region as it appears on file, important for ELF loading.
\item \texttt{mapDirectly} is solely important for programs loaded from bootinfo, which require their text and data sections to be 1:1 mapped to physical memory.
\item \texttt{elffile} is the ELF file the region is located on, represented as a swap file (see Section \ref{sub:demand_paging}).
\end{itemize}

\subsection{Page fault mechanism}

The page fault mechanism is as follows:
\begin{enumerate}
\item The faulting region is located.  If no region is found, or if the type of access which causes the fault (read/write/exec) is illegal, the faulting process is killed (see Section \ref{sub:process_delete}).
\item If the faulting address is on disk, the page fault is added to the ``blocking'' queue to be dealt with asynchronously.  Note that in this case the faulting process is not woken up.
\item If the faulting address needs to be 1:1 mapped, the mapping is immediately made.
\item Otherwise, a frame attempts to be allocated for the fault.  If there are no free frames, the fault is again delayed for later.
\item Assuming nothing is delayed, the reference bit is set, the mapping made, and the faulting process woken.
\end{enumerate}

Specific discussion of the asynchronous paging mechanism, see Section \ref{sub:demand_paging}.

\subsection{Kernel memory}

The kernel is designed in such a way that it assumes it can always allocate frames for itself, a problem when userspace programs are allocated frames from the same pool.  To solve this, the number of userspace frames are artificially limited to a constant amount (\texttt{FRAME\_ALLOC\_LIMIT}), chosen conservatively enough to ensure the kernel will never run out of memory, but generous enough for the limit to only be observable when memory is deliberately thrashed.

This policy has several advantages.  The kernel can run out of physical memory, not virtual, and it requires no complicated IO in order to safely do so.  The kernel can easily allocate temporary frames, useful when creating processes and for ``pinning'' frames to ensure fairness when swapping out.  This also makes testing the memory management easy, by setting the alloc limit very low.

\subsection{Pager thread}

It is worth noting that there are no kernel threads, with the exception of those used for network, initialisation, and \emph{paging}.

The decision to put the pager in its own thread was made for the following reasons:
\begin{itemize}
\item The operations of the pager and the root server are largely\footnote{The root server and pager share a single data structure, the ``copy buffer''.  See Section \ref{sub:system_call_interface}} orthogonal, so operations in the pager can happen simultaneously with the rootserver without race conditions.
\item With its own syscall loop independent from the root servers, the pager is able to IPC the root server.  This is most used in order to use the generic IO system calls, rather than an additional in-kernel interface.  Indeed, this does require that special ``non blocking'' system calls are able to be made, such that the root server's replies arrive in the pager's syscall loop.  Asynchronous IO in the pager (to prevent any blocking) is implemented using this method.
\item It is necessary for the pager to be able to access all of physical memory, most notably for copyin/copyout (see Section \ref{sub:copyin_copyout}).  Without careful management (or a brute force approach), this would cause root task pagefaults.  Instead, this allows the root task to act as the pager for the pager and manage memory accordingly.
\end{itemize}

There are also certain pitfalls:
\begin{itemize}
\item All communication between the pager and root server must be \emph{serialised}, or unexpected behavior and even deadlock can occur.
\item The copy buffer is safe to share as there is one per thread, and threads can only be in one place at a time.  However, no other data structure is necessarily safe.  As a result, several other types of system calls (including all of process management) must be done through the pager.  The division of system call work is discussed further in Section \ref{sub:system_call_interface}.
\end{itemize}

\subsection{Demand paging} \label{sub:demand_paging}

The process of demand paging is slightly complicated, and encompasses nearly all asynchronous requests in the pager, including the lazy ELF loading.  Here just \emph{swapping in} and \emph{swapping out} will be covered, with ELF loading described in Section \ref{sub:elf_loading}.

\subsubsection{Delayed page faults}

As described in the page fault algorithm, there are two cases when a page fault is delayed: if the page is on disk, or if there are no free frames.  These are simply delayed as a ``pager request'' and added to a queue of asynchronous (blocking) requests to be dealt with sequentially.

This asynchronous queue is managed as follows:
\begin{enumerate}
\item Upon adding a request to the queue, if it is currently empty the request is immediately started.  Otherwise it must wait as the pager deals with blocking requests that arrived earlier.
\item When the request is at the head of the queue it is immediately tried again.  This is because frames might have been freed while servicing other requests (for example, if a process died) or the frame might have already been swapped in (for example, if we had implemented shared memory).
\item Assuming the page fault must again block, a sequence of callbacks is started.  A decision is made: if the request simply required a free frame, a \emph{swap out} request is made.  Otherwise, a \emph{swap in} request is made.  These are each described separately below.
\item In either case, when finished the pager request is dequeued.  If the asynchronous request queue is not empty, the next one started and the cycle continues.
\end{enumerate}

The sequential nature of the blocking requests is so that all replies from the root server are unambiguous\footnote{Also, thread safe}, and indeed happen sequentially such that continuation state is simply a counter.  Additionally, while waiting for a reply from the root server, all other non blocking requests to the pager can be immediately dealt.  Several calls to the pager do not necessarily need to block, the most common being copyin or copyout, and this allows them to be immediately serviced.  Incidentally, this also does not introduce artificial overhead as all asynchronous operations involve NFS, which has a fixed bandwidth and attempting multiple requests simultaneously offers no advantage.

\subsubsection{Swapping in}

Swapping in a page involves the following steps:
\begin{itemize}
\item The region list is searched, as each region has its own ``swap file''.  This is a slightly overzealous way to generalise both demand paging and ELF loading as swapping in (in the vein of mmap only weaker), but it works.
\item The page table entry is found for the faulting address.  This contains two releveant pieces of information: the high 24 bits will have been set to the offset of the page in the swap file (\texttt{PAGESIZE} granularity), and the \texttt{SWAP\_MASK} and possible \texttt{ELF\_MASK} bits will have been set.  The difference is discussed in Section \ref{sub:elf_loading}.
\item A temporary frame is allocated to be read in to from the file.  This means a frame will always be available, being out of memory is not an issue.
\item The relevant swap file is opened, to being a chain of continuations.  These will subsequently lseek, read for several blocks\footnote{The size of the blocks are limited by NFS, and in fact 4 iterations are needed to read an entire page}, then close the swap file.
\item When finished, a callback is made - necessary to deal with the differences between completing different types of requests.  For normal swapping in, this callback will free the slot previously taken up on the file.  ELF loading has its own requirements.
\item Finally, if there are spare frames available a permanent one is allocated, the contents from the temporary frame copied in, and the temporary frame freed.  Otherwise a swapout request must be immediately made.
\end{itemize}
\subsubsection{Swapping out}

Swapping a page out involves the following steps:
\begin{itemize}
\item A page is chosen to swap out using the second chance algorithm.  The algorithm is well known so not worth explaining here, but as as reminder the reference bit is kept in the lower 12 bits of every page table entry.  When this bit is unset, the page is also unmapped so that a fault will occur when (or if) later referenced.
\item A slot in the swap file is allocated in a process similar to frame allocation.  This location (as previously mentioned) is stored in the high 24 bits of the page table entry.  Additionally, the \texttt{SWAP\_MASK} bit is set.
\item A swap out request is pushed 
\item The swap out request will open the relevant swap file given in the region list for that entry.  This will actually always be ``.swap'', the .  This begins a chain of continuations which open, lseek, write, then close the file in sequential order.  Writing will actually involve several tries as the NFS buffer is not large enough for an entire page.
\item On finishing: the frame is freed, swap out request dequeued, and the next request run.  This will either be a page fault (which is now guaranteed to find a frame) or a swap in request (likewise, but discussed below).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%
% SYSTEM CALL INTERFACE %
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{System call interface}

\subsection{Copyin/Copyout}

\subsection{VFS Related System Calls} \label{syscalls:vfs}

\begin{verbatim}
/* Open file and return file descriptor, -1 if unsuccessful 
 * (too many open files, console already open for reading).
 * A new file should be created if 'path' does not already exist.
 * A failed attempt to open the console for reading (because it is already
 * open) will result in a context switch to reduce the cost of busy waiting
 * for the console.
 * "path" is file name, "mode" is one of O_RDONLY, O_WRONLY, O_RDWR.
 */
fildes_t open(const char *path, fmode_t mode);

/* A nonblocking version of open which assumes a copyin call has already
 * been made.  Use with caution.
 */
void openNonblocking(const char *path, fmode_t mode);

/* A version of open which allows you to lock the file, specifying the max number of
 * times the file can be opened for reading/writing. Can only lock files which haven't
 * been opened yet. So the fiilile should only be opened using open_lock once and
 * subsequent opens should use plain open.
 */
fildes_t open_lock(const char *path, fmode_t mode, unsigned int readers,
                unsigned int writers);

/* A nonblocking version of open_lock
 */
void open_lockNonblocking(const char *path, fmode_t mode, unsigned int readers,
                unsigned int writers);

/* Closes an open file. Returns 0 if successful, -1 if not (invalid "file").
 */
int close(fildes_t file);

/* Nonblocking version of close */
void closeNonblocking(fildes_t file);

/* Read from an open file, into "buf", max "nbyte" bytes.
 * Returns the number of bytes read.
 * Will block when reading from console and no input is presently
 * available. Returns -1 on error (invalid file).
 */
int read(fildes_t file, char *buf, size_t nbyte);

/* A nonblocking version of read which assumes a copyout call will later
 * be made.  Use with caution.
 */
void readNonblocking(fildes_t file, size_t nbyte);

/* Write to an open file, from "buf", max "nbyte" bytes.
 * Returns the number of bytes written. <nbyte disk is full.
 * Returns -1 on error (invalid file).
 */
int write(fildes_t file, const char *buf, size_t nbyte);

/* A nonblocking version of write which assumes a copyin call has already
 * been made.  Use with caution.
 */
void writeNonblocking(fildes_t file, size_t nbyte);

/* Flush a file or stream out to disk/network
 */
int flush(fildes_t file);

/* Nonblocking version of flush
 */
void flushNonblocking(fildes_t file);

/* Lseek sets the file position indicator to the specified position "pos".
 * if "whence" is set to SEEK_SET, SEEK_CUR, or SEEK_END the offset is relative
 * to the start of the file, current position in the file or end of the file
 * respectively.
 *
 * Returns 0 on success and -1 on error.
 */
int lseek(fildes_t file, fpos_t pos, int whence);

/* Nonblocking version of lseek, use with caution.
 */
void lseekNonblocking(fildes_t file, fpos_t pos, int whence); 

/* Reads name of entry "pos" in directory into "name", max "nbyte" bytes.
 * Returns number of bytes returned, zero if "pos" is next free entry,
 * -1 if error (non-existent entry).
 */
int getdirent(int pos, char *name, size_t nbyte);

/* Returns information about file "path" through "buf".
 * Returns 0 if successful, -1 otherwise (invalid name).
 */
int stat(const char *path, stat_t *buf);

/* Nonblocking version of stat.  Must have done copyin already, and must
 * do copyout afterwards.  Note complication with word alignment.
 */
void statNonblocking(void);

/* Removees the specified file "path".
 * Returns - if successful, -1 otherwise (invalid name).
 */
int fremove(const char *path);

/* Duplicate an open file handler to given a second file handler which points
 * to the same open file. The two file handlers point to the same open file
 * and so share the same offset pointer and open mode.
 */

/* This method returns the first free file descriptor slot found as the duplicate */
int dup(fildes_t file);

/* This method duplicate file to a file descriptor newfile. If newfile is already in
 * use then it is closed.
 */
int dup2(fildes_t file, fildes_t newfile);
\end{verbatim}

\subsection{Process Management} \label{syscalls:process}


%%%%%%%
% VFS %
%%%%%%%

\section{File system} \label{vfs}

The VFS layer implemented in our OS personality provides a flexible and powerful file system handling and as roughly based on the UNIX System V VFS.

\subsection{System Call's Provided} \label{vfs:syscalls}

See section \ref{syscalls:vfs}.

\subsection{Virtual file system structure} \label{vfs:vfs_struct}

The virtual file system (hereafter VFS) uses a standard UNIX three layer structure to provide the features that our OS personality requires. Those layers are as follows below.

\begin{itemize}
\item \texttt{VNodes} are at the core of the VFS providing a standardised interface to all the file systems supported by our OS personality. An individual \texttt{VNode} represents one 'file' (whether that be an actual file or a device) and provides functions to manipulate the file. \texttt{VNodes} are created as needed and stored on a single linked list of all open \texttt{VNodes} in the system. \texttt{VNodes} are global and not tied to any particular process and can be shared concurrently among them.
\item \texttt{VFiles} are the second layer of the VFS, they are tied to a particular process and used to represent the state of an open file, such as the permissions it was opened with and the current offset into the file. \texttt{VFiles} are tied to a \texttt{VNode} and it is through them that one \texttt{VNode} can be opened numerous times since they separate out any process specific state. \texttt{VFiles} are stored in a fixed size array inside of the PCB which limits the maximum number of files which a process can open in our system to 16.
\item \texttt{File Descriptors} are used as a key and identifier for a particular \texttt{VFile}. They are used in system calls to work with the VFS. Like \texttt{VFiles} they are unique to a particular process and are stored in a fixed size array inside of the PCB. File descriptors are not a necessary feature but allow for the extensions to our OS, such as dup, which are detailed in section \ref{vfs:extensions}
\end{itemize}

While our OS supports are reasonable amount of the standard UNIX System V VFS functionality, it lacks some significant features as detailed below:
\begin{itemize}
\item No support for directories. The support directory structure is a single unified space.
\item No support for dynamic file systems. This includes operations such as mounting and unmounting file systems at runtime. Because of this the relation between the implemented file systems is fixed. This is in detailed section \ref{vfs:filesystems}
\end{itemize}

\subsection{\texttt{VNode} structure} \label{vfs:vnode_struct}

\texttt{VNodes} are implemented in the code as a struct, which is detailed below. These are managed with a linked list. Call-backs are used in many of the \texttt{VNode} operations to allow for file systems to flexibly call the VFS layer to report the end of their operations. This is required as the file system only deal with \texttt{VNodes} and after most operations the \texttt{VFile} and \texttt{File Descriptor} layers must be modified appropriately.

\begin{verbatim}
struct VNode_t {
    char path[MAX_FILE_NAME];
    stat_t vstat;
    
    unsigned int Max_Readers;
    unsigned int Max_Writers;
    unsigned int readers;
    unsigned int writers;
    
    void *extra; 
    
    VNode previous;
    VNode next;
    
    void (*open)(pid_t pid, VNode self, const char *path, fmode_t mode,
        void (*open_done)(pid_t pid, VNode self, fmode_t mode, int status));
    
    void (*close)(pid_t pid, VNode self, fildes_t file, fmode_t mode,
        void (*close_done)(pid_t pid, VNode self, fildes_t file, fmode_t mode, int status));
    
    void (*read)(pid_t pid, VNode self, fildes_t file, L4_Word_t pos,
        char *buf, size_t nbyte, void (*read_done)(pid_t pid, VNode self,
            fildes_t file, L4_Word_t pos, char *buf, size_t nbyte, int status));
    
    void (*write)(pid_t pid, VNode self, fildes_t file, L4_Word_t offset,
        const char *buf, size_t nbyte, void (*write_done)(pid_t pid, VNode self,
            fildes_t file, L4_Word_t offset, const char *buf, size_t nbyte, int status));
    
    void (*flush)(pid_t pid, VNode self, fildes_t file);
    
    void (*getdirent)(pid_t pid, VNode self, int pos, char *name, size_t nbyte);
    
    void (*stat)(pid_t pid, VNode self, const char *path, stat_t *buf);
    
    void (*remove)(pid_t pid, VNode self, const char *path);
};
\end{verbatim}

\begin{itemize}
\item \texttt{path} stores the path (file name) of the file this \texttt{VNode} represents.
\item \texttt{vstat} stores statistics about the \texttt{VNode} such as on disk permissions, size and last accessed timestamps.
\item \texttt{Max\_Readers} specifies an upper limit on the number of times this particular \texttt{VNode} can be opened in read mode. Specified by the file system itself or through the system call. This enables an extension feature of locking files (detailed in section \ref{vfs:extensions}).
\item \texttt{Max\_Writers} is the same as Max\_Readers but for write mode opens.
\item \texttt{readers} stores a reference count of the number of \texttt{VFiles} currently linked to this \texttt{VNode} with read permissions.
\item \texttt{writers} is the same as readers but for write permissions.
\item \texttt{extra} allows for individual file systems to attach their own specific data to a \texttt{VNode}.
\item \texttt{previous} \texttt{VNodes} are stored one a double linked list.
\item \texttt{next} see previous.
\item \texttt{open} function pointer to file system specific function to open the \texttt{VNode}. Currently not used.
\item \texttt{close} function pointer to file system specific function to close the \texttt{VNode}.
\item \texttt{read} function pointer to file system specific function to read data from the \texttt{VNode}.
\item \texttt{write} function pointer to file system specific function to write data to the \texttt{VNode}.
\item \texttt{flush} function pointer to file system specific function to flush out any cache for the \texttt{VNode} to the back device.
\item \texttt{getdirent} function pointer to file system specific function to list the contents of the \texttt{VNode}. Since directories aren't supported, this is currently not used.
\item \texttt{stat} function pointer to file system specific function to get the stat information about this \texttt{VNode}.
\item \texttt{remove} function pointer to file system specific function to remove (delete/unlink) this \texttt{VNode} from its file system.
\end{itemize}

\subsection{\texttt{VFile} structure} \label{vfs:vfile_struct}

A \texttt{VFile} is implemented as a struct and collectively they are managed per process in a fix size array inside of the processes \texttt{PCB}.

\begin{verbatim}
typedef struct {
    VNode vnode;
    fmode_t fmode;
    L4_Word_t fp;
	 L4_Word_t ref;
} VFile;
\end{verbatim}

\begin{itemize}
\item \texttt{vnode} stores a pointer to the \texttt{VNode} that this file is linked to.
\item \texttt{fmode} stores the access mode that the file was opened with. This is checked against for read and write operations.
\item \texttt{fp} stores the current offset into the file.
\item \texttt{ref} stores the number of times this \texttt{VFile} has been referenced. Please see section \ref{vfs:file_ops} for details of how \texttt{ref} is used.
\end{itemize}

\subsection{File Descriptors} \label{vfs:fds}

File descriptors are simply a c int type and are stored as an array in a processes \texttt{PCB}. They store the array element number of the \texttt{VFile} that they refer to.

\subsection{File Operations} \label{vfs:file_ops}

\subsubsection{Opening and Closing Files} \label{vfs:file_ops:openclose}

The file opening mechanism is as follows:

\begin{enumerate}
\item The file name and mode are checked to ensure they are valid.
\item An empty file descriptor and \texttt{VFile} slot are found in their respective arrays in the calling processes \texttt{PCB}. If a slot for either can't be found, then the operation finishes unsuccessfully.
\item The open global \texttt{VNode} list is searched to see if the file is already currently open, if so the next step is invoked. Otherwise the NFS file system is invoked to attempt to open the specified file. If it is successful, then the next step is invoked. Otherwise the operation finishes unsuccessfully.
\item At this stage we have successfully retrieved a \texttt{VNode} for the specified file. Firstly the permissions of the \texttt{VNode} are checked to ensure that it is allowed to be opened in the mode the process specified, the Maximum reference counters are also checked to ensure this file can be opened by additional processes. Secondly the \texttt{VFile} and file descriptor are setup appropriately. The \texttt{VNodes} reference counts are also increased appropriately, if it is a new \texttt{VNode} then they will have been initialised to zero. The reference count on the \texttt{VFile} is set to one.
\item The operation returns successfully, informing the calling process of the file descriptor which can be used to access the new file.
\end{enumerate}

The file closing mechanism is as follows:

\begin{enumerate}
\item First all the system call parameters are checked to ensure they are valid and refer to an open file.
\item The \texttt{VFile} is retrieved using the information from the file descriptor passed in with the system call. The reference count of the \texttt{VFile} is reduced by one and if it is now at zero then it is closed and the next step is invoked. Otherwise, the operation returns successfully.
\item The appropriate reference count(s) of the \texttt{VNode} are reduced and if both of them are now zero, then the \texttt{VNodes} close operation is called. After this is finished, the \texttt{VNode} itself is closed and removed from the global list.
\end{enumerate}

\subsubsection{Reading and Writing} \label{vfs:read_write}

Reading and writing operations are carried out as per usual with the one exception being the \texttt{console} file system. The \texttt{console} file system implements an additional feature of buffering. This is done to avoid corruptions which were occurring due to the underlying network code being unable to handle a common case of the console rapidly sending individual characters. The details of this are detailed further in section \ref{vfs:console}

\subsection{Supported File Systems} \label{vfs:filesystems}

Our OS personality supports two file systems. These are NFS and a Console file system.

\subsubsection{Console File System} \label{vfs:console}

The console file system is initialised by the VFS layer at boot time and it immediately adds one \texttt{VNode} with the file name \texttt{"console"} to the open \texttt{VNode} list. This ensures that VFS operations on the file \texttt{"console"} will be passed through to the console file system. The console file system only supports this one console device file largely due to limitations with the provided network and serial driver code, the console file system itself could easily support multiple devices. The console file system also implements basic buffering support as first outlined in section \ref{vfs:read_write}. A small buffer of 64 bytes is used, although this is a configurable parameter. The console file is flushed automatically once it becomes full, when a new line ('\textbackslash n') character is encountered, or before a read system call is invoked. It can also be flushed manually as needed though the appropriate system call as detailed in section \ref{vfs:syscalls}.

\subsection{VFS Extensions} \label{vfs:extensions}

Our VFS layer has a number of extensions as compared with the required implementation specification. These can be seen through the following extra support system calls as outlined in section \ref{vfs:syscalls} and briefly below:

\begin{itemize}
\item \texttt{int remove(const char* path)} Allows for files to be deleted form the file system. Open files are not allowed to be removed from the system.
\item \texttt{int dup(fildes\_t fd)} and \texttt{dup2(fildes\_t fd, fildes\_t new\_fd)} Allows a file descriptor to be duplicated, functions as per usual POSIX dup/dup2 specifications with full support, including closing the file is new\_fd is already in use.
\item \texttt{int lseek(fildes\_t file, fpos\_t pos, int whence)} Allows the file position pointer on a file to be modified, functions as per usual POSIX specification with full support.
\item \texttt{int flush(fildes\_t file)} Flushes the specified files buffer out to its backing device. This is only supported by the console file system and functions as specified in section \ref{vfs:console}.
\end{itemize}

\subsubsection{I/O Redirection} \label{vfs:redirect}

A major extension of our VFS in addition to those outlined above is the support for I/O redirection. When a process is created, the standard in, standard error and standard input file streams can be specified if desired rather then using the system defaults. The system call interface for this is detailed in section \ref{syscalls:process}. The files to redirect to (all VFS file systems are supported) is first opened by the parent process, which then creates the process passing through the file descriptors. The parent process can close the files though, there is no dependency on it for the proper functioning of I/O redirection as the files are then reopened by the child process.

For redirecting standard input, the VFS layer stores a flag in the child processes PCB detailing that I/O redirection for standard input is active. When this is the case, any open call to "console" which is the way in which standard input is opened according to the specification provided, is ignored and instead the redirected standard input file descriptor is returned. This allows for some exciting features in our OS personality such as the ability to script SOSH by creating a new SOSH sub-process which reads from a file as opposed to from console.


%%%%%%%%%%%%%%%%%%%%%%%
%% PROCESS MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%%%

\section{Process management}

\end{document}

