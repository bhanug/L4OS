\documentclass[12pt,english]{article}
\usepackage{geometry}
\geometry{a4paper,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
\usepackage{babel}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{hyperref}

\hypersetup{
    pdftitle={Advanced Operating Systems - System Documentation},
    pdfauthor={Group 3: David Terei, Benjamin Kalman},
    pdfkeywords={Awesome},
    pdfborder={0 0 0}
}

\begin{document}

%%%%%%%%%%%%%
% TITLE/TOC %
%%%%%%%%%%%%%

\title{Advanced Operating Systems - System Documentation}
\author{\textbf{Group 3} \\ David Terei, Benjamin Kalman}
\maketitle

\tableofcontents{}

%%%%%%%%%%%%%%%%%%%%%
% MEMORY MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%

\newpage{}
\section{Memory management}

\subsection{Page table structure}

The page table is a lazy, two level page table.  

\begin{verbatim}
#define PAGEWORDS (PAGESIZE / sizeof(L4_Word_t))

typedef struct  {
    L4_Word_t pages[PAGEWORDS];
} Pagetable2;

typedef struct {
    Pagetable2 *pages2[PAGEWORDS];
} Pagetable1;
\end{verbatim}

\subsection{Address location}

To locate the 32 bit virtual address \texttt{v}, the highest 12 bits are used to index in to the first level of the table (\texttt{Pagetable1}) in order to find the corresponding second level struct.  If this is \texttt{NULL}, this second level is dynamically created.  The middle 12 bits of \texttt{v} are then used to index in to this second level, locating the relevant entry \texttt{e}.  Note that the lowest 12 bits of \texttt{v} are not needed for page table addressing.

The high 24 bits of \texttt{e} give the \emph{page aligned} physical address\footnote{In reality, an L4 ppage.} of what \texttt{v} is mapped to.  The lower 12 bits are then used to maintain status bits, for example the reference bit (0x1), swapped status (0x2), or ELF mask (0x4).

\subsection{Region management}

Regions are managed with a linked list of \texttt{Region} structs.

\begin{verbatim}
struct Region_t {
    region_type type;
    uintptr_t base;
    unsigned int size;
    unsigned int filesize;
    int rights;
    int mapDirectly;
    Swapfile *elffile;
};
\end{verbatim}

\begin{itemize}
\item \texttt{type} is used to differentiate between the stack, heap, and other regions.  It is important to know which region is the heap in order for the \texttt{moremem} system call to function.
\item \texttt{base}, \texttt{size}, and \texttt{rights} are the self explanitory.
\item \texttt{filesize} is the size of the region as it appears on file, important for ELF loading.
\item \texttt{mapDirectly} is solely important for programs loaded from bootinfo, which require their text and data sections to be 1:1 mapped to physical memory.
\item \texttt{elffile} is the ELF file the region is located on, represented as a swap file (see Section \ref{sub:demand_paging}).
\end{itemize}

\subsection{Page fault mechanism}

The page fault mechanism is as follows:
\begin{enumerate}
\item The faulting region is located.  If no region is found, or if the type of access which causes the fault (read/write/exec) is illegal, the faulting process is killed (see Section \ref{sub:process_delete}).
\item If the faulting address is on disk, the page fault is added to the ``blocking'' queue to be dealt with asynchronously.  Note that in this case the faulting process is not woken up.
\item If the faulting address needs to be 1:1 mapped, the mapping is immediately made.
\item Otherwise, a frame attempts to be allocated for the fault.  If there are no free frames, the fault is again delayed for later.
\item Assuming nothing is delayed, the reference bit is set, the mapping made, and the faulting process woken.
\end{enumerate}

For specific discussion of the asynchronous paging mechanism, see Section \ref{sub:demand_paging}.

\subsection{Kernel memory}

The kernel is designed in such a way that it assumes it can always allocate frames for itself, a problem when userspace programs are allocated frames from the same pool.  To solve this, the number of userspace frames are artificially limited to a constant amount (\texttt{FRAME\_ALLOC\_LIMIT}), chosen conservatively enough to ensure the kernel will never run out of memory, but generous enough for the limit to only be observable when memory is deliberately thrashed.

This policy has several advantages.  The kernel can run out of physical memory, not virtual, and it requires no complicated IO in order to safely do so.  The kernel can easily allocate temporary frames, useful when creating processes and for ``pinning'' frames to ensure fairness when swapping out.  This also makes testing the memory management easy, by setting the alloc limit very low.

\subsection{Pager thread}

It is worth noting that there are no kernel threads, with the exception of those used for network, initialisation, and \emph{paging}.

The decision to put the pager in its own thread was made for the following reasons:
\begin{itemize}
\item The operations of the pager and the root server are largely\footnote{The root server and pager share a single data structure, the ``copy buffer'', see Section \ref{sub:system_call_interface}.} orthogonal, so operations in the pager can happen simultaneously with the rootserver without race conditions.
\item With its own syscall loop independent from the root servers, the pager is able to IPC the root server.  This is most used in order to use the generic IO system calls, rather than an additional in-kernel interface.  Indeed, this does require that special ``non blocking'' system calls are able to be made, such that the root server's replies arrive in the pager's syscall loop.  Asynchronous IO in the pager (to prevent any blocking) is implemented using this method.
\item It is necessary for the pager to be able to access all of physical memory, most notably for copyin/copyout (see Section \ref{sub:copyin_copyout}).  Without careful management (or a brute force approach), this would cause root task pagefaults.  Instead, this allows the root task to act as the pager for the pager and manage memory accordingly.
\end{itemize}

There are also certain pitfalls:
\begin{itemize}
\item All communication between the pager and root server must be \emph{serialised}, or unexpected behavior and even deadlock can occur.
\item The copy buffer is safe to share as there is one per thread, and threads can only be in one place at a time.  However, no other data structure is necessarily safe.  As a result, several other types of system calls (including all of process management) must be done through the pager.  The division of system call work is discussed further in Section \ref{sub:system_call_interface}.
\end{itemize}

\subsection{Demand paging} \label{sub:demand_paging}

The process of demand paging is slightly complicated, and encompasses nearly all asynchronous requests in the pager, including the lazy ELF loading.  Here just \emph{swapping in} and \emph{swapping out} will be covered, with ELF loading described in Section \ref{sub:elf_loading}.

\subsubsection{Delayed page faults}

As described in the page fault algorithm, there are two cases when a page fault is delayed: if the page is on disk, or if there are no free frames.  These are simply delayed as a ``pager request'' and added to a queue of asynchronous (blocking) requests to be dealt with sequentially.

This asynchronous queue is managed as follows:
\begin{enumerate}
\item Upon adding a request to the queue, if it is currently empty the request is immediately started.  Otherwise it must wait as the pager deals with blocking requests that arrived earlier.
\item When the request is at the head of the queue it is immediately tried again.  This is because frames might have been freed while servicing other requests (for example, if a process died) or the frame might have already been swapped in (for example, if we had implemented shared memory).
\item Assuming the page fault must again block, a sequence of callbacks is started.  A decision is made: if the request simply required a free frame, a \emph{swap out} request is made.  Otherwise, a \emph{swap in} request is made.  These are each described separately below.
\item In either case, when finished the pager request is dequeued.  If the asynchronous request queue is not empty, the next one started and the cycle continues.
\end{enumerate}

The sequential nature of the blocking requests is so that all replies from the root server are unambiguous\footnote{Also, thread safe.}, and indeed happen sequentially such that continuation state is simply a counter.  Additionally, while waiting for a reply from the root server, all other non blocking requests to the pager can be immediately dealt.  Several calls to the pager do not necessarily need to block, the most common being copyin or copyout, and this allows them to be immediately serviced.  Incidentally, this also does not introduce artificial overhead as all asynchronous operations involve NFS, which has a fixed bandwidth and attempting multiple requests simultaneously offers no advantage.

\subsubsection{Swapping in}

Swapping in a page involves the following steps:
\begin{itemize}
\item The region list is searched, as each region has its own ``swap file''.  This is a slightly overzealous way to generalise both demand paging and ELF loading as swapping in (in the vein of mmap only weaker), but it works.
\item The page table entry is found for the faulting address.  This contains two releveant pieces of information: the high 24 bits will have been set to the offset of the page in the swap file (\texttt{PAGESIZE} granularity), and the \texttt{SWAP\_MASK} and possible \texttt{ELF\_MASK} bits will have been set.  The difference is discussed in Section \ref{sub:elf_loading}.
\item A temporary frame is allocated to be read in to from the file.  This means a frame will always be available, and being out of memory is not an issue.
\item The relevant swap file is opened and begins a chain of continuations.  These will subsequently lseek, read for several blocks\footnote{The size of the blocks are limited by NFS, and 4 iterations are needed to read an entire page.}, then close the swap file.
\item When finished, a callback is made - necessary to deal with the differences between completing different types of requests.  For normal swapping in, this callback will free the slot previously taken up on the file.  ELF loading has its own requirements.
\item Finally, if there are spare frames available a permanent one is allocated, the contents from the temporary frame copied in, and the temporary frame freed.  Otherwise a swapout request must be immediately made.
\end{itemize}
\subsubsection{Swapping out}

Swapping a page out involves the following steps:
\begin{itemize}
\item A page is chosen to swap out using the second chance algorithm.  The algorithm is well known so not worth explaining here, but as as reminder the reference bit is kept in the lower 12 bits of every page table entry.  When this bit is unset, the page is also unmapped so that a fault will occur when (or if) later referenced.
\item A slot in the swap file is allocated in a process similar to frame allocation.  This location (as previously mentioned) is stored in the high 24 bits of the page table entry.  Additionally, the \texttt{SWAP\_MASK} bit is set.
\item A swap out request is pushed to the head of the queue and started by opening the default swap file (``.swap'' rather than the one in the region list - an unfortunate hack).  Like swapping in, a chain of continuations is processed.
\item On finishing, the frame is free.  Normally the next request is simply dequeued and started (which will be the original pager request), however there is slightly more to do if the swap out was in response to a swap in.  In this case the old temporary frame still exists, so its contents must be copied in to the recently freed frame, \emph{then} the next request run.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%
% SYSTEM CALL INTERFACE %
%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage{}
\section{System call interface} \label{sub:system_call_interface}

\subsection{Inter-process communication protocol}

The kernel uses a simple IPC protocol: the message tag contains the type of system call, and message registers contain the system call parameters.  For large amounts of data, communication is made through the copyin/copyout mechanism, explained in Section \ref{sub:copyin_copyout}.

The number of system calls is extensive, although many are for debugging and diagnostic purposes:

\begin{verbatim}
typedef enum {
        SOS_REPLY,
        SOS_KERNEL_PRINT,
        SOS_DEBUG_FLUSH,
        SOS_MOREMEM,
        SOS_COPYIN,
        SOS_COPYOUT,
        SOS_OPEN,
        SOS_CLOSE,
        SOS_READ,
        SOS_FLUSH,
        SOS_WRITE,
        SOS_LSEEK,
        SOS_GETDIRENT,
        SOS_STAT,
        SOS_REMOVE,
        SOS_PROCESS_CREATE,
        SOS_PROCESS_DELETE,
        SOS_MY_ID,
        SOS_PROCESS_STATUS,
        SOS_PROCESS_WAIT,
        SOS_TIME_STAMP,
        SOS_USLEEP,
        SOS_MEMUSE,
        SOS_SWAPUSE,
        SOS_PHYSUSE,
        SOS_VPAGER,
        SOS_MEMLOC,
        SOS_MMAP,
        SOS_SHARE_VM,
        L4_PAGEFAULT = ((L4_Word_t) -2),
        L4_INTERRUPT = ((L4_Word_t) -1),
        L4_EXCEPTION = ((L4_Word_t) -5)
} syscall_t;
\end{verbatim}

\subsection{Division of work}

As already mentioned, there are two separate entities capable of handling system calls: the root server, and the pager.  The division is made such that the set of calls handled by each do not interfere with one another.

\subsection{Copyin/Copyout} \label{sub:copyin_copyout}

A significant feature for all library code is the copyin/copyout policy used for most system calls.  Whenevever a significant\footnote{Any more than a few words is classed as ``significant''.} amount of data needs to be transferred between user programs and the kernel, the \texttt{copyin}/\texttt{SOS\_COPYIN} or associated \texttt{copyout}/\texttt{SOS\_COPYOUT} system call is made.  These copy data from the user in to a kernel-maintained buffer (copyin), or from the kernel buffer to the user (copyout).

\begin{verbatim}
void copyin(void *data, size_t size, int append);
void copyout(void *data, size_t size, int append);
\end{verbatim}

For example, when calling \texttt{write} in libsos, the user is in fact issuing a \texttt{SOS\_COPYIN} for the data to write, then a \texttt{SOS\_WRITE} with relevant parameters to write out that buffer.  Similarly, when calling \texttt{read} in libsos a \texttt{SOS\_READ} is made, followed by \texttt{SOS\_COPYOUT} to copy the data in to the user's address space.

\subsubsection{Interface}

In both cases, \texttt{data} is a pointer to the data in the user's address space, \texttt{size} is the size of the data in bytes, and \texttt{append} indicates whether this data should be appended to what is currently in the kernel's buffer, or overwrite it.  Both system calls are made to the pager thread, not the rootserver.

\subsubsection{Implementation}

The kernel buffer is a static array of a predetermined size, currently twice the page size.  There is one buffer per thread, but given user threads aren't actually supported this amounts to 256 separate buffers, for a total memory cost of approximately 2MB.

On receiving a \texttt{SOS\_COPYIN} system call, the following steps are made:
\begin{enumerate}
\item The parameters of the copy are set up - this in fact requires extra static memory containing the size and offset of the operation.  Depending on the value of \texttt{append}, the offset may be left at its current value, or reset to 0.
\item A page fault is ``forced'', by manufacturing a pager request and setting the \texttt{finished} callback to the actual copyin operation.  Conveniently, the page fault is the very same mechanism discussed in Section \ref{sub:demand_paging} such that all the convoluted continuations involved in swapping in and out (and indeed ELF loading, if applicable) all happen with barely any extra complexity.  When the pagefault is completed, the copying begins.
\item The user's page table is looked up for the actual physical address the relevant virtual address is mapped to.
\item Since the page fault has brought in the page, and since nothing has occurred in the mean time, the user's page is \emph{guaranteed} to be immediately accessible.  So, data is copied in from the address up to the desired size, \emph{or} until a page boundary is reached.
\item If the former, the system call simply returns.  If the latter, a page fault is forced with the same process as previously occurred.  This loop continues until the buffer is full or the all the data has been copied in.
\end{enumerate}

Copy out is implemented in the identical way.  Additionally, the rights of the copyout are easily checked by setting the rights of the pagefault to write - such that if the user violates this (i.e. if they are writing to the text segment for example) they will be killed.  Ruthless but correct.

The rights of the copyin are also checked, but it is unlikely that a region be without read permissions (unless it doesn't exist, in which case the process is also killed).

\subsubsection{Advantages}

The copyin/copyout design has several advantages, most of which are already implied above:
\begin{itemize}
\item It is safe, in that the kernel (that is, pager) will never block while performing a copy, and never do anything illegal from the user's perspective.
\item Given that the kernel owns the data in its own buffer, it can be modified at its discretion.  Not only is this nice and (thread) safe, it is especially useful when ``spoofing'' system calls from users that would have otherwise required a copyin or copyout.
\item Many of the limitations mentioned below are excusable given the small number of buffers actually needed.
\end{itemize}

\subsubsection{Limitations}

There are, of course, several limitations:
\begin{itemize}
\item There is a constant 1MB of memory unavailable for general use.
\item There is a finite size to the amount of data that can be stored by the kernel (however, it is certainly large enough for most purposes).
\item Both copyin and copyout share the same buffer, so the order of operations is important.  Particularly, \texttt{printf} will initiate a \texttt{copyin} so any debugging attempted to be made anywhere in this process could cause unexpected results.
\item Care needs to be taken to word align data in the buffers.
\end{itemize}


%%%%%%%
% VFS %
%%%%%%%

\newpage{}
\section{File system} \label{vfs}

The VFS layer implemented in our OS personality provides a flexible and powerful file system handling and as roughly based on the UNIX System V VFS.

\subsection{Virtual file system structure} \label{vfs:vfs_struct}

The virtual file system (hereafter VFS) uses a standard UNIX three layer structure to provide the features that our OS personality requires. Those layers are as follows below.

\begin{itemize}
\item \texttt{VNodes} are at the core of the VFS providing a standardised interface to all the file systems supported by our OS personality. An individual \texttt{VNode} represents one 'file' (whether that be an actual file or a device) and provides functions to manipulate the file. \texttt{VNodes} are created as needed and stored on a single linked list of all open \texttt{VNodes} in the system. \texttt{VNodes} are global and not tied to any particular process and can be shared concurrently among them.
\item \texttt{VFiles} are the second layer of the VFS, they are tied to a particular process and used to represent the state of an open file, such as the permissions it was opened with and the current offset into the file. \texttt{VFiles} are tied to a \texttt{VNode} and it is through them that one \texttt{VNode} can be opened numerous times since they separate out any process specific state. \texttt{VFiles} are stored in a fixed size array inside of the PCB which limits the maximum number of files which a process can open in our system to 16.
\item \texttt{File Descriptors} are used as a key and identifier for a particular \texttt{VFile}. They are used in system calls to work with the VFS. Like \texttt{VFiles} they are unique to a particular process and are stored in a fixed size array inside of the PCB. File descriptors are not a necessary feature but allow for the extensions to our OS, such as dup, which are detailed in section \ref{vfs:extensions}
\end{itemize}

While our OS supports are reasonable amount of the standard UNIX System V VFS functionality, it lacks some significant features as detailed below:
\begin{itemize}
\item No support for directories. The support directory structure is a single unified space.
\item No support for dynamic file systems. This includes operations such as mounting and unmounting file systems at runtime. Because of this the relation between the implemented file systems is fixed. This is in detailed section \ref{vfs:filesystems}
\end{itemize}

\subsection{\texttt{VNode} structure} \label{vfs:vnode_struct}

\texttt{VNodes} are implemented in the code as a struct, which is detailed below. These are managed with a linked list. Call-backs are used in many of the \texttt{VNode} operations to allow for file systems to flexibly call the VFS layer to report the end of their operations. This is required as the file system only deal with \texttt{VNodes} and after most operations the \texttt{VFile} and \texttt{File Descriptor} layers must be modified appropriately.

\begin{verbatim}
struct VNode_t {
    char path[MAX_FILE_NAME];
    stat_t vstat;
    
    unsigned int Max_Readers;
    unsigned int Max_Writers;
    unsigned int readers;
    unsigned int writers;
    
    void *extra; 
    
    VNode previous;
    VNode next;
    
    void (*open)(pid_t pid, VNode self, const char *path, fmode_t mode,
        void (*open_done)(pid_t pid, VNode self, fmode_t mode, int status));
    
    void (*close)(pid_t pid, VNode self, fildes_t file, fmode_t mode,
        void (*close_done)(pid_t pid, VNode self, fildes_t file, fmode_t mode, int status));
    
    void (*read)(pid_t pid, VNode self, fildes_t file, L4_Word_t pos,
        char *buf, size_t nbyte, void (*read_done)(pid_t pid, VNode self,
            fildes_t file, L4_Word_t pos, char *buf, size_t nbyte, int status));
    
    void (*write)(pid_t pid, VNode self, fildes_t file, L4_Word_t offset,
        const char *buf, size_t nbyte, void (*write_done)(pid_t pid, VNode self,
            fildes_t file, L4_Word_t offset, const char *buf, size_t nbyte, int status));
    
    void (*flush)(pid_t pid, VNode self, fildes_t file);
    
    void (*getdirent)(pid_t pid, VNode self, int pos, char *name, size_t nbyte);
    
    void (*stat)(pid_t pid, VNode self, const char *path, stat_t *buf);
    
    void (*remove)(pid_t pid, VNode self, const char *path);
};
\end{verbatim}

\begin{itemize}
\item \texttt{path} stores the path (file name) of the file this \texttt{VNode} represents.
\item \texttt{vstat} stores statistics about the \texttt{VNode} such as on disk permissions, size and last accessed timestamps.
\item \texttt{Max\_Readers} specifies an upper limit on the number of times this particular \texttt{VNode} can be opened in read mode. Specified by the file system itself or through the system call. This enables an extension feature of locking files (detailed in section \ref{vfs:extensions}).
\item \texttt{Max\_Writers} is the same as Max\_Readers but for write mode opens.
\item \texttt{readers} stores a reference count of the number of \texttt{VFiles} currently linked to this \texttt{VNode} with read permissions.
\item \texttt{writers} is the same as readers but for write permissions.
\item \texttt{extra} allows for individual file systems to attach their own specific data to a \texttt{VNode}.
\item \texttt{previous} \texttt{VNodes} are stored one a double linked list.
\item \texttt{next} see previous.
\item \texttt{open} function pointer to file system specific function to open the \texttt{VNode}. Currently not used.
\item \texttt{close} function pointer to file system specific function to close the \texttt{VNode}.
\item \texttt{read} function pointer to file system specific function to read data from the \texttt{VNode}.
\item \texttt{write} function pointer to file system specific function to write data to the \texttt{VNode}.
\item \texttt{flush} function pointer to file system specific function to flush out any cache for the \texttt{VNode} to the back device.
\item \texttt{getdirent} function pointer to file system specific function to list the contents of the \texttt{VNode}. Since directories aren't supported, this is currently not used.
\item \texttt{stat} function pointer to file system specific function to get the stat information about this \texttt{VNode}.
\item \texttt{remove} function pointer to file system specific function to remove (delete/unlink) this \texttt{VNode} from its file system.
\end{itemize}

\subsection{\texttt{VFile} structure} \label{vfs:vfile_struct}

A \texttt{VFile} is implemented as a struct and collectively they are managed per process in a fix size array inside of the processes \texttt{PCB}.

\begin{verbatim}
typedef struct {
    VNode vnode;
    fmode_t fmode;
    L4_Word_t fp;
    L4_Word_t ref;
} VFile;
\end{verbatim}

\begin{itemize}
\item \texttt{vnode} stores a pointer to the \texttt{VNode} that this file is linked to.
\item \texttt{fmode} stores the access mode that the file was opened with. This is checked against for read and write operations.
\item \texttt{fp} stores the current offset into the file.
\item \texttt{ref} stores the number of times this \texttt{VFile} has been referenced. Please see section \ref{vfs:file_ops} for details of how \texttt{ref} is used.
\end{itemize}

\subsection{File Descriptors} \label{vfs:fds}

File descriptors are simply a c int type and are stored as an array in a processes \texttt{PCB}. They store the array element number of the \texttt{VFile} that they refer to.

\subsection{File Operations} \label{vfs:file_ops}

\subsubsection{Opening and Closing Files} \label{vfs:file_ops:openclose}

The file opening mechanism is as follows:

\begin{enumerate}
\item The file name and mode are checked to ensure they are valid.
\item An empty file descriptor and \texttt{VFile} slot are found in their respective arrays in the calling processes \texttt{PCB}. If a slot for either can't be found, then the operation finishes unsuccessfully.
\item The open global \texttt{VNode} list is searched to see if the file is already currently open, if so the next step is invoked. Otherwise the NFS file system is invoked to attempt to open the specified file. If it is successful, then the next step is invoked. Otherwise the operation finishes unsuccessfully.
\item At this stage we have successfully retrieved a \texttt{VNode} for the specified file. Firstly the permissions of the \texttt{VNode} are checked to ensure that it is allowed to be opened in the mode the process specified, the Maximum reference counters are also checked to ensure this file can be opened by additional processes. Secondly the \texttt{VFile} and file descriptor are setup appropriately. The \texttt{VNodes} reference counts are also increased appropriately, if it is a new \texttt{VNode} then they will have been initialised to zero. The reference count on the \texttt{VFile} is set to one.
\item The operation returns successfully, informing the calling process of the file descriptor which can be used to access the new file.
\end{enumerate}

The file closing mechanism is as follows:

\begin{enumerate}
\item First all the system call parameters are checked to ensure they are valid and refer to an open file.
\item The \texttt{VFile} is retrieved using the information from the file descriptor passed in with the system call. The reference count of the \texttt{VFile} is reduced by one and if it is now at zero then it is closed and the next step is invoked. Otherwise, the operation returns successfully.
\item The appropriate reference count(s) of the \texttt{VNode} are reduced and if both of them are now zero, then the \texttt{VNodes} close operation is called. After this is finished, the \texttt{VNode} itself is closed and removed from the global list.
\end{enumerate}

\subsubsection{Reading and Writing} \label{vfs:read_write}

Reading and writing operations are carried out as per usual with the one exception being the \texttt{console} file system. The \texttt{console} file system implements an additional feature of buffering. This is done to avoid corruptions which were occurring due to the underlying network code being unable to handle a common case of the console rapidly sending individual characters. The details of this are detailed further in section \ref{vfs:console}

\subsection{Supported File Systems} \label{vfs:filesystems}

Our OS personality supports two file systems. These are NFS and a Console file system.

\subsubsection{Console File System} \label{vfs:console}

The console file system is initialised by the VFS layer at boot time and it immediately adds one \texttt{VNode} with the file name \texttt{"console"} to the open \texttt{VNode} list. This ensures that VFS operations on the file \texttt{"console"} will be passed through to the console file system. The console file system only supports this one console device file largely due to limitations with the provided network and serial driver code, the console file system itself could easily support multiple devices. The console file system also implements basic buffering support as first outlined in section \ref{vfs:read_write}. A small buffer of 64 bytes is used, although this is a configurable parameter. The console file is flushed automatically once it becomes full, when a new line ('\textbackslash n') character is encountered, or before a read system call is invoked. It can also be flushed manually as needed though the appropriate system call as detailed in section \ref{vfs:extensions}.

\subsubsection{NFS File System} \label{vfs:nfs}

The NFS file system implementation uses a continuation based, single threaded design with an queue mechanism for handling multiple threads and object orientated data structures. Each NFS File uses the \texttt{extra} void pointer of the respective \texttt{VNode} to store the following additional structure:

\begin{verbatim}
typedef struct {
    VNode vnode;
    struct cookie fh;
} NFS_File;
\end{verbatim}

\begin{itemize}
\item \texttt{vnode} Stores a pointer back to the VNode this \texttt{NFS\_File} is attached to.
\item \texttt{fh} Stores the NFS file cookie for talking to the NFS server.
\end{itemize}

Each request for a VFS operation (open, close, read, write...) is stored on a single global queue as a generic \texttt{NFS\_BaseRequest} struct/object, however this is a reduced data type used to handle each request type generally using object orientated techniques of polymorphism, the individual data structures are all detailed as follows:

\begin{verbatim}
struct NFS_BaseRequest_t {
   enum NfsRequestType rt;
   uintptr_t token;
   VNode vnode;
   pid_t pid;
};

typedef struct {
   NFS_BaseRequest p;
   fmode_t mode;
   void (*open_done) (pid_t pid, VNode self, fmode_t mode, int status);
} NFS_LookupRequest;

typedef struct {
   NFS_BaseRequest p;
   fildes_t file;
   char *buf;
   L4_Word_t pos;
   size_t nbyte;
   void (*read_done)(pid_t pid, VNode self, fildes_t file, L4_Word_t pos, char *buf,
         size_t nbyte, int status);
} NFS_ReadRequest;

typedef struct {
   NFS_BaseRequest p;
   fildes_t file;
   char *buf;
   L4_Word_t offset;
   size_t nbyte;
   void (*write_done)(pid_t pid, VNode self, fildes_t file, L4_Word_t offset,
         const char *buf, size_t nbyte, int status);
} NFS_WriteRequest;

typedef struct {
   NFS_BaseRequest p;
   stat_t *stat;
   const char *path;
} NFS_StatRequest;

typedef struct {
   NFS_BaseRequest p;
   int pos;
   char *buf;
   size_t nbyte;
   int cpos;
} NFS_DirRequest;

typedef struct {
   NFS_BaseRequest p;
   const char *path;
} NFS_RemoveRequest;
\end{verbatim}

By making sure each request struct stores a \texttt{NFS\_BaseRequest} as its first piece of data we can cast them all to a \texttt{NFS\_BaseRequest} and use the \texttt{NfsRequestType} parameter to cast them back. The token parameter of \texttt{NFS\_BaseRequest} is used to uniquely identify each request and is passed around in all the call backs so that the correct request can be found in the request queue. One request is processed at a time to avoid corruption issues occurring as the underlying network and NFS libraries can not handle multiple concurrent requests.

\subsection{VFS Extensions} \label{vfs:extensions}

Our VFS layer has a number of extensions as compared with the required implementation specification. These can be seen through the following extra support system calls as outlined briefly below:

\begin{itemize}
\item \texttt{int remove(const char* path)} Allows for files to be deleted form the file system. Open files are not allowed to be removed from the system.
\item \texttt{int dup(fildes\_t fd)} and \texttt{dup2(fildes\_t fd, fildes\_t new\_fd)} Allows a file descriptor to be duplicated, functions as per usual POSIX dup/dup2 specifications with full support, including closing the file is new\_fd is already in use.
\item \texttt{int lseek(fildes\_t file, fpos\_t pos, int whence)} Allows the file position pointer on a file to be modified, functions as per usual POSIX specification with full support.
\item \texttt{int flush(fildes\_t file)} Flushes the specified files buffer out to its backing device. This is only supported by the console file system and functions as specified in section \ref{vfs:console}.
\end{itemize}

\subsubsection{I/O Redirection} \label{vfs:redirect}

A major extension of our VFS in addition to those outlined above is the support for I/O redirection. When a process is created, the standard in, standard error and standard input file streams can be specified if desired rather then using the system defaults. The system call interface for this is as follows:

\begin{verbatim}
pid_t process_create(const char *path, fildes_t stdout, fildes_t stderr, fildes_t stdin);
\end{verbatim}

The files to redirect to (all VFS file systems are supported) is first opened by the parent process, which then creates the process passing through the file descriptors. The parent process can close the files though as there is no dependency on it for the proper functioning of I/O redirection since the files are then reopened by the child process.

For redirecting standard input, the VFS layer stores a flag in the child processes PCB detailing that I/O redirection for standard input is active. When this is the case, any open call to "console" which is the way in which standard input is opened according to the specification provided, is ignored and instead the redirected standard input file descriptor is returned. This allows for some exciting features in our OS personality such as the ability to script SOSH by creating a new SOSH sub-process which reads from a file as opposed to from console. Standard out and error are redirected by opening the appropriate files with the correct file descriptors so that calls to \texttt{printf} and \texttt{write(stdout\_fd} will be directed to the appropriate files.


%%%%%%%%%%%%%%%%%%%%%%%
%% PROCESS MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%%%

\newpage{}
\section{Process management}

\subsection{Process control block}

All data for a single process is stored in a per-process control block\footnote{With the exception of the copy buffers, which are static and so not in the PCB.} (PCB).

\begin{verbatim}
typedef struct {
    pid_t             pid; // pid of the process
    unsigned          size; // size in pages
    unsigned          stime; // microseconds since booting
    char              command[MAX_FILE_NAME]; // path to the executable
    process_state_t   state; // state (start, alive, sleep, wait, zombie)
    process_type_t    ps_type; // type (process, kernel thread)
    process_ipcfilt_t ipc_accept; // accepted IPCs (nonblocking, blocking, all)
} process_t;

struct Process_t {
    process_t     info; // the information exported to the user
    Pagetable    *pagetable; // two level page table
    List         *regions; // region list
    void         *sp; // stack pointer, only used on startup
    void         *ip; // instruction pointer, only used on startup
    timestamp_t   startedAt; // time the process was started
    VFile         files[PROCESS_MAX_FILES]; // all open files
    pid_t         waitingOn; // the process this one is waiting on
};
\end{verbatim}

\subsection{Process creation and ELF loading} \label{sub:elf_loading}

Process creation involves ELF loading - it also may involve loading from the boot image, however this is simple and not particularly interesting so won't be covered.

Much of the mechanism used for ELF loading has already been covered, as it has been seen as a variation of a swap file and dealt with as such.  There are two distinct stages: the initial parsing of the ELF header, and the subsequent lazy loading of the pages from the executable as needed.

\subsubsection{ELF header parsing}

ELF header parsing is done on the same asynchronous request queue in the pager as pager requests are done.  The relevant struct is:

\begin{verbatim}
typedef struct {
    elfload_stage_t stage; // stage in series of callbacks
    char path[MAX_FILE_NAME]; // path to executable
    fildes_t fd; // file descriptor (when opened)
    pid_t parent; // pid of the process that made the request
    pid_t child; // pid of the process created
} ElfloadRequest;
\end{verbatim}

When an ELF load request comes to the head of the pager's request queue, a sequence of continuations is initiated:
\begin{enumerate}
\item The ELF file is opened.
\item A \texttt{stat} is run on the file to check it has execute permissions in-kernel.  The necessity of this is debatable.
\item The ELF header is read in to the pager's copy buffer.  Since the exact size of the header is effectively unknown (albeit known to be quite small), exactly one pass is made.  This is enough to read in over a kilobyte, certainly enough data.  The copy buffer itself can then be cast to an \texttt{Elf32\_Header} and dealt with directly.
\item Using information from the ELF file via the elf libraries, the PCB of the new processes is filled in as necessary.  Most importantly, the page table entries are pre-filled to have the correct disk offset and masks (\texttt{SWAP\_MASK | ELF\_MASK}).  The region list is also set up to have the ELF file as the swap file for each region\footnote{In hindsight, yes, it would have made more sense to do this differently.}.
\item The ELF file is closed, and caller of \texttt{process\_create} woken.
\end{enumerate}

\subsubsection{ELF section lazy loading}

It would be a perfect situation if the ELF sections loaded themselves in the exact way a swap file does, however there are a few small but significant differences.
\begin{itemize}
\item An ELF files needs not concept of ``free slots''.
\item Indeed, an ELF file should never actually be swapped out to at all, as writing over an ELF file (for example, if the data section were to be swapped out) could have strange consequences.
\item An ELF file has a small gap in memory between the region in memory and the region on disk, and this needs to be zeroed.
\end{itemize}

To solve these problems, the lazy loading of ELF files was implemented as a generalisation of swapping in, with an extra callback and a few special cases.
\begin{itemize}
\item As previously mentioned, the callback for a ``true'' swap in would deallocate a slot in the swapfile.  In the case of ELF loading, the callback needs to zero out the aforementioned area of memory.  It must also unset the \texttt{ELF\_MASK} flag from the page table entries.
\item To prevent an ELF file from being swapped out to, the pager will never actually consider the swap file in the region list.  This presents a problem if the data section of a process was swapped out to the default swap file, then read back in from the ELF file.  To solve this, the pager will look at the swap file from the region list only if the \texttt{ELF\_MASK} is set for a page table entry.
\item In actual fact, this could allow easily for read-only section optimisations, and even dirty bit optimisations, but these were not implemented.
\end{itemize}

\subsection{Process deletion} \label{sub:process_delete}

Process deletion involves two steps: killing the process itself, and freeing its resources.  The steps are:

\begin{enumerate}
\item The process is killed (\texttt{L4\_ThreadControl}) and all L4-related data is freed.
\item The process is put in a zombie state.  This enforces the kernel not to pass through replies to that process, useful when spoofing the \texttt{vfs\_close} calls.
\item All files open by the process are flushed and closed.  This frees up all VFS-related data.
\item The in-kernel resources are freed:
\begin{itemize}
\item Allocated frames
\item Slots in the swap file
\item Page table
\item Regions and associated list
\end{itemize}
\item All processes waiting on either any process or this specific process are woken, see below.  
\item The PCB is freed.  The pid will then become available for the next process.
\end{enumerate}

\subsection{Process waiting}

When a process calls \texttt{process\_wait}/\texttt{SOS\_PROCESS\_WAIT}, the \texttt{waitingOn} is set to the desired process.  When any process is deleted, all PCB's are searched for any processes that need to be woken up.  Slightly inefficient perhaps, but very clean and simple.

\end{document}

