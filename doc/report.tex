\documentclass[12pt,english]{article}
\usepackage{geometry}
\geometry{a4paper,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\usepackage{setspace}
\onehalfspacing
\usepackage{babel}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{amsmath}

\begin{document}

%%%%%%%%%%%%%
% TITLE/TOC %
%%%%%%%%%%%%%

\title{Advanced Operating Systems - System Documentation}
\author{\textbf{Group 3} \\ David Terei, Benjamin Kalman}
\maketitle

\tableofcontents{}

%%%%%%%%
% TODO %
%%%%%%%%

\newpage{}
\section{TODO}

\begin{itemize}
\item 3: system call interface
\item 4: NFS
\item 5: demand paging
\item 6: timer driver?
\item 7: process management
\item 8: ELF loading
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%
% MEMORY MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%

\section{Memory management}

\subsection{Page table structure}

The page table is a lazy, two level page table.  

\begin{verbatim}
#define PAGEWORDS (PAGESIZE / sizeof(L4_Word_t))

typedef struct  {
    L4_Word_t pages[PAGEWORDS];
} Pagetable2;

typedef struct {
    Pagetable2 *pages2[PAGEWORDS];
} Pagetable1;
\end{verbatim}

\subsection{Address location}

To locate the 32 bit virtual address \texttt{v}, the highest 12 bits are used to index in to the first level of the table (\texttt{Pagetable1}) in order to find the corresponding second level struct.  If this is \texttt{NULL}, this second level is dynamically created.  The middle 12 bits of \texttt{v} are then used to index in to this second level, locating the relevant entry \texttt{e}.  Note that the lowest 12 bits of \texttt{v} are not needed for page table addressing.

The high 24 bits of \texttt{e} give the \emph{page aligned} physical address\footnote{In fact, ppage} of what \texttt{v} is mapped to.  The lower 12 bits are then used to maintain status bits, for example the reference bit and swapped status.

\subsection{Region management}

Regions are managed with a linked list of \texttt{Region} structs.

\begin{verbatim}
struct Region_t {
    region_type type;
    uintptr_t base;
    unsigned int size;
    unsigned int filesize;
    int rights;
    int mapDirectly;
    Swapfile *elffile;
};
\end{verbatim}

\begin{itemize}
\item \texttt{type} is used to differentiate between the stack, heap, and other regions.  It is important to know which region is the heap in order for the \texttt{moremem} system call to function, see Section \ref{sub:moremem}.
\item \texttt{base}, \texttt{size}, and \texttt{rights} are the self explanitory.
\item \texttt{filesize} is the size of the region as it appears on file, important for ELF loading.
\item \texttt{mapDirectly} is solely important for programs loaded from bootinfo, which require their text and data sections to be 1:1 mapped to physical memory.
\item \texttt{elffile} is the ELF file the region is located on, represented as a swap file (see Section \ref{sub:demand_paging}).
\end{itemize}

\subsection{Page fault mechanism}

The page fault mechanism is as follows:
\begin{enumerate}
\item The faulting region is located.  If no region is found, or if the type of access which causes the fault (read/write/exec) is illegal, the faulting process is killed (see Section \ref{sub:process_delete}).
\item If the faulting address is on disk, the page fault is added to the ``blocking'' queue to be dealt with asynchronously.  Note that in this case the faulting process is not woken up.
\item If the faulting address needs to be 1:1 mapped, the mapping is immediately made.
\item Otherwise, a frame attempts to be allocated for the fault.  If there are no free frames, the fault is again delayed for later.
\item Assuming nothing is delayed, the reference bit is set, the mapping made, and the faulting process woken.
\end{enumerate}

Specific discussion of the asynchronous paging mechanism, see Section \ref{sub:demand_paging}.

\subsection{Kernel memory}

The kernel is designed in such a way that it assumes it can always allocate frames for itself, a problem when userspace programs are allocated frames from the same pool.  To solve this, the number of userspace frames are artificially limited to a constant amount (\texttt{FRAME\_ALLOC\_LIMIT}), chosen conservatively enough to ensure the kernel will never run out of memory, but generous enough for the limit to only be observable when memory is deliberately thrashed.

This policy has several advantages.  The kernel can run out of physical memory, not virtual, and it requires no complicated IO in order to safely do so.  The kernel can easily allocate temporary frames, useful when creating processes and for ``pinning'' frames to ensure fairness when swapping out.  This also makes testing the memory management easy, by setting the alloc limit very low.

\subsection{Pager thread}

It is worth noting that there are no kernel threads, with the exception of those used for network, initialisation, and \emph{paging}.

The decision to put the pager in its own thread was made for the following reasons:
\begin{itemize}
\item The operations of the pager and the root server are largely\footnote{The root server and pager share a single data structure, the ``copy buffer''.  See Section \ref{sub:system_call_interface}} orthogonal, so operations in the pager can happen simultaneously with the rootserver, (theoretically) without race conditions.
\item With its own syscall loop independent from the root servers, the pager is able to IPC the root server.  This is most used in order to use the generic IO system calls, rather than an additional in-kernel interface.  Indeed, this does require that special ``non blocking'' system calls are able to be made, such that the root server's replies arrive in the pager's syscall loop.  Overall, this is used to implement the asynchronous IO used in the pager.
\item It is necessary for the pager to be able to access all of physical memory, most notably for copyin/copyout (see Section \ref{sub:copyin_copyout}).  Without careful management (or a brute force approach), this would cause root task pagefaults.  Instead, this allows the root task to act as the pager for the pager and manage memory accordingly.
\end{itemize}

Several other types of system calls are also handled by the pager, including all process management system calls.  This is necessary as process management is closely linked to memory management, and race conditions arise otherwise.  The division of system call work is discussed further in Section \ref{sub:system_call_interface}.

\subsection{Demand paging} \label{sub:demand_paging}

The process of demand paging is slightly complicated, and encompasses nearly all asynchronous requests in the pager, including the lazy ELF loading.  For now however, just \emph{swapping in} and \emph{swapping out} will be covered, with ELF loading described in Section \ref{sub:elf_loading}.

\subsubsection{Delayed page faults}

As described in the page fault algorithm, there are two cases when a page fault is delayed: if the page is on disk, or if there are no free frames.  These are simply delayed as a ``pager request'' and added to a queue of asynchronous (blocking) requests to be dealt with sequentially.

This asynchronous queue is managed as follows:
\begin{enumerate}
\item Upon adding a request to the queue, if it is currently empty the request at the head is immediately started.
\item When started, the original page fault request is attempted again.  In the meantime, a frame might have been freed (for example, if a process died) or the frame might have already been swapped in (for example, if we had implemented shared memory).
\item Assuming the page fault must again block, a sequence of callbacks is immediately started.  A decision is made: if the request simply required a free frame, a \emph{swap out} request is made.  Otherwise, a \emph{swap in} request is made.  These are each described separately below.
\item In either case, upon finishin the page request is dequeued.  If the asynchronous request queue is not empty then the next one started.
\end{enumerate}

The sequential nature of the blocking requests is so that all replies from the root server are unambiguous, and indeed happen sequentially such that continuation state is simply a counter.  There is no blocking, as while waiting for a reply from the root server all other requests to the pager.  Incidentally, this also does not introduce artificial overhead as all asynchronous operations involve NFS, which has a fixed bandwidth and attempting multiple requests simultaneously offers no advantage.

\subsubsection{Swapping out}

Swapping a page out involves the following steps:
\begin{itemize}
\item A page is chosen to swap out using the second chance algorithm.  The algorithm is well known so not worth explaining here, but as as reminder the reference bit is kept in the lower 12 bits of every page table entry.  When this bit is unset, the page is also unmapped so that a fault will occur when (or if) later referenced.
\item A \emph{swap out request} is pushed to the heap of the request queue, then the request started.
\item A slot in the swap file is allocated in a process similar to frame allocation.
\item The swap out request will open (non blocking) the relevant swap file given in the region list (although currently always ``.swap'').  This sets of a chain of continuations which open, lseek, write, then close the file in sequential order.
\item On finishing: the frame is freed, swap out request dequeued, and the next request run.  This will either be a page fault (which is now guaranteed to find a frame) or a swap in request (likewise, but discussed below).
\end{itemize}

\subsubsection{Swapping in}

Swapping in a page is slightly more complicated than swapping in a page:
\begin{itemize}
\item The region list is searched for the faulting 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%
% SYSTEM CALL INTERFACE %
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{System call interface}

\subsection{Copyin/Copyout}

%%%%%%%
% VFS %
%%%%%%%

\section{File system}


\subsection{Virtual file system structure}

The virtual file system (hereafter VFS) uses a standard UNIX three layer structure to provide the features that our OS personality requires. Those layers are as follows below.

\begin{itemize}
\item \texttt{VNodes} are at the core of the VFS providing a standardised interface to all the file systems supported by our OS. An individual VNode represents one file (whether that be an actual file or a device) and provides functions to manipulate the file. Vnodes are created to abstract a file as needed and stored on a single linked list of all open vnodes in the system. VNodes are global and not tied to any particular process.
\item \texttt{VFiles} are the second layer of the VFS, they are tied to a partciular process and used to represent the state of an open file, such as the permissions it was opened with and the current offset into the file. VFiles are tied to a VNode and it is through them that one VNode can be opened numerous times. VFiles are stored in a fixed size array inside of the PCB.
\item \texttt{File Descriptors} are used as a key and identifier for a particular VFile, used in syscalls to work with the VFS. Like VFiles they are unique to a particular process and are stored in a fixed size array inside of the PCB. File descriptors are necessary but allow for the extensions to our OS detailed at the end of this section.
\end{itemize}

Our OS differs from standard UNIX VFS in that it only supports a flat directory structure, directories are not support in any form. There is also no support for dealing with file systems such as mount operations, instead the currently supported file systems have been hard coded into the VFS.

\subsection{\texttt{VNode} structure}

\begin{verbatim}
struct VNode_t {
	char path[MAX_FILE_NAME];
	stat_t vstat;

	unsigned int Max_Readers;
	unsigned int Max_Writers;
	unsigned int readers;
	unsigned int writers;
	
	void *extra; 

	VNode previous;
	VNode next;

	void (*open)(pid_t pid, VNode self, const char *path, fmode_t mode,
			void (*open_done)(pid_t pid, VNode self, fmode_t mode, int status));

	void (*close)(pid_t pid, VNode self, fildes_t file, fmode_t mode,
			void (*close_done)(pid_t pid, VNode self, fildes_t file, fmode_t mode, int status));

	void (*read)(pid_t pid, VNode self, fildes_t file, L4_Word_t pos,
			char *buf, size_t nbyte, void (*read_done)(pid_t pid, VNode self,
				fildes_t file, L4_Word_t pos, char *buf, size_t nbyte, int status));

	void (*write)(pid_t pid, VNode self, fildes_t file, L4_Word_t offset,
			const char *buf, size_t nbyte, void (*write_done)(pid_t pid, VNode self,
				fildes_t file, L4_Word_t offset, const char *buf, size_t nbyte, int status));

	void (*flush)(pid_t pid, VNode self, fildes_t file);

	void (*getdirent)(pid_t pid, VNode self, int pos, char *name, size_t nbyte);

	void (*stat)(pid_t pid, VNode self, const char *path, stat_t *buf);

	void (*remove)(pid_t pid, VNode self, const char *path);
};
\end{verbatim}

\begin{itemize}
\item \texttt{path} stores the path (file name) of the file this VNode represents.
\item \texttt{vstat} stores statistics about the vnode such as on disk permissions, size and last accessed timestamps.
\item \texttt{Max\_Readers} specifies an upper limit on the number of times this particular VNode can be opened in read mode. Specified by the filesystem itself or through the system call. This enables an extension feature of locking files (detailed in the VFS Extensions section).
\item \texttt{Max\_Writers} is the same as Max\_Readers but for write mode opens.
\item \texttt{readers} stores a reference count of the number of VFiles currently linked to this VNode with read permissions.
\item \texttt{writers} is the same as readers but for write permissions.
\item \texttt{extra} allows for individual file systems to attach their own specific data to a VNode.
\item \texttt{previous} VNodes are stored one a double linked list.
\item \texttt{next} see previous.
\item \texttt{open} function pointer to file system specific function to open the VNode. Currently not used.
\item \texttt{close} function pointer to file system specific function to close the VNode.
\item \texttt{read} function pointer to file system specific function to read data from the VNode.
\item \texttt{write} function pointer to file system specific function to write data to the VNode.
\item \texttt{flush} function pointer to file system specific function to flush out any cache for the vnode to the back device.
\item \texttt{getdirent} function pointer to file system specific function to list the contents of the VNode. Since directories aren't supported, this is currently not used.
\item \texttt{stat} function pointer to file system specific function to get the stat information about this VNode.
\item \texttt{remove} function pointer to file system specific function to remove (delete/unlink) this VNode from its file system.
\end{itemize}

\subsection{\texttt{VFile} structure}

\begin{verbatim}
typedef struct {
	VNode vnode;
	fmode_t fmode;
	L4_Word_t fp;
} VFile;
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%
% PROCESS MANAGEMENT %
%%%%%%%%%%%%%%%%%%%%%%

\section{Process management}

\end{document}

